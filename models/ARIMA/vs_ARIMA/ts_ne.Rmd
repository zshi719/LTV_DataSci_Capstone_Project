---
title: "Time Series"
output: html_document
date: "2023-11-04"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = F, warning = F)
library(tidyverse)
library(ggfortify)
library(forecast)
```

# Data importing and cleaning

We firstly load the data and convert the confirmed cases to ts objects:

```{r}
df <- read.csv("ne_train.csv", row.names = 1)
getwdconfirmed <- ts(df$confirmed, start = c(4, 4), frequency = 7)
confirmed

```

Note that here we assume a week is a 'season', i.e. we assume there may be patterns in some specific days of a week.

# Data visualization

The plot of confirmed cases clearly are not stationary and there is some seasonality.

```{r}
autoplot(confirmed) +
  ylab("Retail index") +
  xlab("Week")
```

Thus, we first take a seasonal difference and the plot of resulting series, ACF and PACF are illustrated in the figure below. However, the series after a seasonal difference still does not seem to be stationary, so we take an additional first difference.

```{r}
confirmed %>% diff(lag = 7) %>% ggtsdisplay()
```

The double differenced data seem to be stationary as indicated in the graph below. The significant spike at lag 2 in the ACF suggests a non-seasonal MA(2) component, and the significant spike at lag 7 in the ACF suggests a seasonal MA(1) component. Consequently, we begin with an $ARIMA(0,1,2)(0,1,1)_7$ model, indicating a first and seasonal difference, and non-seasonal MA(2) component and seasonal MA(1) components.

```{r}
confirmed %>%
  diff(lag = 7) %>%
  diff() %>%
  ggtsdisplay()
```

# Seasonal ARIMA

## Model fitting

Now we fit $ARIMA(0,1,2)(0,1,1)_7$ model to the data and the residuals are plotted in the following figure. We see that there are spikes that exceed the significance limits, so the residuals do not appear to be white noise. The Ljung-Box test also shows that the residuals may have remaining autocorrelations. Thus, we need to modify the model further.

```{r}
fit1 <- Arima(confirmed, order = c(0, 1, 2), seasonal = c(0, 1, 1))
checkresiduals(fit1)
```

We run the `auto.arima` function to see which setup of the model gives the lowest AIC and BIC:

```{r}
auto.arima(confirmed, seasonal = TRUE)
```

We see that model $ARIMA(2,1,2)(0,0,1)_7$ gives the lowest AIC and BIC. We can plot the residuals and conduct Ljung-Box test:

```{r}
fit2 <- Arima(confirmed, order = c(2, 1, 2), seasonal = c(0, 0, 1))
checkresiduals(fit2)
```

The result suggests that there is weak autocorrelation of residuals in our model.

## Forecasting

Load the test data:

```{r}
df_test <- read.csv("ne_test.csv", row.names = 1)
confirmed_test <- ts(df_test$confirmed, start = c(31, 6), frequency = 7)
confirmed_test
```

```{r}
fit2 %>% forecast(h = 48) %>% autoplot()
```

Compute RMSE for test data:

```{r}
pred <- data.frame(forecast(fit2, h = 48))[, 1]
# RMSE
sqrt(mean((pred - confirmed_test)^2))
```

# Seasonal ARIMA with Log transformation

```{r}
log_confirmed <- log(confirmed + 1)
auto.arima(log_confirmed, seasonal = TRUE)
fit3 <- arima(log_confirmed, order = c(2, 2, 2))
checkresiduals(fit3)
```

```{r}
# make predictions
pred3 <- exp(data.frame(forecast(fit3, h = 48))[, 1]) - 1
# RMSE
sqrt(mean((pred3 - confirmed_test)^2))
```

We see that the AIC and BIC after transformation is much smaller than the original model. However, this may be caused by the magnitude has been squeezed after transformation, and the prediction RMSE is even larger when we transform back to the original scale. Thus, the log transformation does not give a better prediction.

# ARIMA with exogenous variables (ARIMA-X)

```{r}
df_sub <- df %>%
  select(confirmed, daily_state_test, precipitation, temperature, weekend)
df_sub %>% GGally::ggpairs()
```

# model fitting

```{r}
# Loading time series data and predictors
df_sub$confirmed <- ts(df_sub$confirmed, start = c(4, 4), frequency = 7)
df_sub$daily_state_test <- ts(df_sub$daily_state_test, start = c(4, 4), frequency = 7)
df_sub$precipitation <- ts(df_sub$precipitation, start = c(4, 4), frequency = 7)
df_sub$temperature <- ts(df_sub$temperature, start = c(4, 4), frequency = 7)
attach(df_sub)

# fit will all exogenous variables available
multi.fit1 <- auto.arima(confirmed, xreg = cbind(daily_state_test, precipitation, temperature, weekend))
summary(multi.fit1)
```

```{r}
checkresiduals(multi.fit1)
```

Figure above shows a time plot, the ACF and the histogram of the residuals from the multiple regression model fitted to the confirmed cases data, as well as the Breusch-Godfrey test for jointly testing up to 8th order autocorrelation. The time plot shows some changing variation over time. This heteroscedasticity will potentially make the prediction interval coverage inaccurate. The histogram shows that the residuals seem to be skewed, which may also affect the coverage probability of the prediction intervals.

Thus, we search for other better fitting models by reducing exogenous variables:

```{r}
multi.fit2 <- auto.arima(confirmed, xreg = cbind(daily_state_test, precipitation, temperature))
multi.fit3 <- auto.arima(confirmed, xreg = cbind(precipitation, temperature))
multi.fit4 <- auto.arima(confirmed, xreg = cbind(daily_state_test, precipitation))
multi.fit5 <- auto.arima(confirmed, xreg = cbind(precipitation))
data.frame(
  model = c("multi.fit1", "multi.fit2", "multi.fit3", "multi.fit4", "multi.fit5"),
  AIC = c(multi.fit1$aic, multi.fit2$aic, multi.fit3$aic, multi.fit4$aic, multi.fit5$aic),
  BIC = c(multi.fit1$bic, multi.fit2$bic, multi.fit3$bic, multi.fit4$bic, multi.fit5$bic))
```

```{r}
checkresiduals(multi.fit2)$p.value
checkresiduals(multi.fit3)$p.value
checkresiduals(multi.fit4)$p.value
checkresiduals(multi.fit5)$p.value
```

Considering the autocorrelation and AIC/BIC, the final model selected is fit5, that is, precipitation as the exogenous variable only.

Below is a plot of true value and fitted values:

```{r}
fitted_confirmed <- fitted(multi.fit5)
confirmed_df <- data.frame(
  time = 1:length(confirmed),
  value = c(confirmed, fitted_confirmed),
  type = c(rep("true", length(confirmed)), rep("forecasts", length(fitted_confirmed)))
)
ggplot(confirmed_df, aes(x = time, y = value, col = as.factor(type))) +
  geom_line(size = 0.75) +
  labs(y = "Y", x = "Time",
       title = "Fitted values against actual, 01/22/2020 - 07/30/2020") +
  theme(axis.title = element_text(family = "serif"),
        plot.title = element_text(hjust = 0.5, size = 12, family = "serif", face = "bold"),
        legend.position = c(0.85, 0.2),
        legend.title = element_blank(),
        legend.text = element_text(family = "serif", face = "bold"),
        legend.key = element_rect(color = "transparent"),
        legend.background = element_rect(fill = "lightgrey")) +
  scale_color_discrete(name = "Lead", labels = c("Forecast", " Actual Value"))
```

We see that the model fits well.

## Forecasting

```{r}
pred.fit5 <- data.frame(forecast(multi.fit5, xreg = df_test$precipitation))[, 1]
# RMSE
sqrt(mean((pred.fit5 - confirmed_test)^2))
```

```{r}
pred_df <- data.frame(
  time = 1:length(df_test$confirmed),
  value = c(df_test$confirmed, pred.fit5),
  type = c(rep("true", length(df_test$confirmed)), rep("forecasts", length(pred.fit5)))
)
ggplot(pred_df, aes(x = time, y = value, col = as.factor(type))) +
  geom_line(size = 0.75) +
  labs(y = "Y", x = "Time",
       title = "Forecast values against actual, 07/31/2020 - 09/16/2020") +
  theme(axis.title = element_text(family = "serif"),
        plot.title = element_text(hjust = 0.5, size = 12, family = "serif", face = "bold"),
        legend.position = c(0.85, 0.2),
        legend.title = element_blank(),
        legend.text = element_text(family = "serif", face = "bold"),
        legend.key = element_rect(color = "transparent"),
        legend.background = element_rect(fill = "lightgrey")) +
  scale_color_discrete(name = "Lead", labels = c("Forecast", " Actual Value"))
```

However, the RMSE of the new model is not significantly less than the seasonal ARIMA without exogenous variables. Further improvements can be made by adding and selecting more effective exogenous variables.
